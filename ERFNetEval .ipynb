{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ERFNetEval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GZDwa6M5sCQ"
      },
      "source": [
        "# connect google"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBh__3akDSz7"
      },
      "source": [
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vo3UKi_te1E"
      },
      "source": [
        "! npm install -g localtunnel\n",
        "# 8097 is the port number I set myself, which can be modified to the port number I want to use\n",
        "get_ipython().system_raw('python3 -m pip install visdom')\n",
        "get_ipython().system_raw('python3 -m visdom.server -port 8097 >> visdomlog.txt 2>&1 &')   \n",
        "get_ipython().system_raw('lt --port 8097 >> url.txt 2>&1 &')   \n",
        "import time\n",
        "time.sleep(5)\n",
        "! cat url.txt\n",
        "import visdom\n",
        "time.sleep(5)\n",
        "vis = visdom.Visdom(port='8097')  \n",
        "print(vis)\n",
        "time.sleep(3)\n",
        "vis.text('testing')\n",
        "! cat visdomlog.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEfRGAEA51P_"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSFqLJQF55tt"
      },
      "source": [
        "# Code with dataset loader for VOC12 and Cityscapes (adapted from bodokaiser/piwise code)\n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "EXTENSIONS = ['.jpg', '.png']\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "\n",
        "def is_image(filename):\n",
        "    return any(filename.endswith(ext) for ext in EXTENSIONS)\n",
        "\n",
        "def is_label(filename):\n",
        "    return filename.endswith(\"_labelTrainIds.png\")\n",
        "\n",
        "def image_path(root, basename, extension):\n",
        "    return os.path.join(root, f'{basename}{extension}')\n",
        "\n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def image_basename(filename):\n",
        "    return os.path.basename(os.path.splitext(filename)[0])\n",
        "\n",
        "class VOC12(Dataset):\n",
        "\n",
        "    def __init__(self, root, input_transform=None, target_transform=None):\n",
        "        self.images_root = os.path.join(root, 'images')\n",
        "        self.labels_root = os.path.join(root, 'labels')\n",
        "\n",
        "        self.filenames = [image_basename(f)\n",
        "            for f in os.listdir(self.labels_root) if is_image(f)]\n",
        "        self.filenames.sort()\n",
        "\n",
        "        self.input_transform = input_transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.filenames[index]\n",
        "\n",
        "        with open(image_path(self.images_root, filename, '.jpg'), 'rb') as f:\n",
        "            image = load_image(f).convert('RGB')\n",
        "        with open(image_path(self.labels_root, filename, '.png'), 'rb') as f:\n",
        "            label = load_image(f).convert('P')\n",
        "\n",
        "        if self.input_transform is not None:\n",
        "            image = self.input_transform(image)\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "\n",
        "class cityscapes(Dataset):\n",
        "\n",
        "    def __init__(self, root, input_transform=None, target_transform=None, subset='val'):\n",
        "        self.images_root = os.path.join(root, 'leftImg8bit/' + subset)\n",
        "        self.labels_root = os.path.join(root, 'gtFine/' + subset)\n",
        "\n",
        "        self.filenames = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.images_root)) for f in fn if is_image(f)]\n",
        "        self.filenames.sort()\n",
        "\n",
        "        self.filenamesGt = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.labels_root)) for f in fn if is_label(f)]\n",
        "        self.filenamesGt.sort()\n",
        "\n",
        "        self.input_transform = input_transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.filenames[index]\n",
        "        filenameGt = self.filenamesGt[index]\n",
        "\n",
        "        #print(filename)\n",
        "\n",
        "        with open(image_path_city(self.images_root, filename), 'rb') as f:\n",
        "            image = load_image(f).convert('RGB')\n",
        "        with open(image_path_city(self.labels_root, filenameGt), 'rb') as f:\n",
        "            label = load_image(f).convert('P')\n",
        "\n",
        "        if self.input_transform is not None:\n",
        "            image = self.input_transform(image)\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label, filename, filenameGt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeZY7vn56GOo"
      },
      "source": [
        "# erfnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nydwsSco6L4a"
      },
      "source": [
        "# ERFNET full network definition for Pytorch\n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DownsamplerBlock (nn.Module):\n",
        "    def __init__(self, ninput, noutput):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n",
        "        self.pool = nn.MaxPool2d(2, stride=2)\n",
        "        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = torch.cat([self.conv(input), self.pool(input)], 1)\n",
        "        output = self.bn(output)\n",
        "        return F.relu(output)\n",
        "    \n",
        "\n",
        "class non_bottleneck_1d (nn.Module):\n",
        "    def __init__(self, chann, dropprob, dilated):        \n",
        "        super().__init__()\n",
        "\n",
        "        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n",
        "\n",
        "        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n",
        "\n",
        "        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n",
        "\n",
        "        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropprob)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output = self.conv3x1_1(input)\n",
        "        output = F.relu(output)\n",
        "        output = self.conv1x3_1(output)\n",
        "        output = self.bn1(output)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output = self.conv3x1_2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.conv1x3_2(output)\n",
        "        output = self.bn2(output)\n",
        "\n",
        "        if (self.dropout.p != 0):\n",
        "            output = self.dropout(output)\n",
        "        \n",
        "        return F.relu(output+input)    #+input = identity (residual connection)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.initial_block = DownsamplerBlock(3,16)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(DownsamplerBlock(16,64))\n",
        "\n",
        "        for x in range(0, 5):    #5 times\n",
        "           self.layers.append(non_bottleneck_1d(64, 0.1, 1))  \n",
        "\n",
        "        self.layers.append(DownsamplerBlock(64,128))\n",
        "\n",
        "        for x in range(0, 2):    #2 times\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 2))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 4))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 8))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 16))\n",
        "\n",
        "        #only for encoder mode:\n",
        "        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, input, predict=False):\n",
        "        output = self.initial_block(input)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "\n",
        "        if predict:\n",
        "            output = self.output_conv(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class UpsamplerBlock (nn.Module):\n",
        "    def __init__(self, ninput, noutput):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n",
        "        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        output = self.bn(output)\n",
        "        return F.relu(output)\n",
        "\n",
        "class Decoder (nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(UpsamplerBlock(128,64))\n",
        "        self.layers.append(non_bottleneck_1d(64, 0, 1))\n",
        "        self.layers.append(non_bottleneck_1d(64, 0, 1))\n",
        "\n",
        "        self.layers.append(UpsamplerBlock(64,16))\n",
        "        self.layers.append(non_bottleneck_1d(16, 0, 1))\n",
        "        self.layers.append(non_bottleneck_1d(16, 0, 1))\n",
        "\n",
        "        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "\n",
        "        output = self.output_conv(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class ERFNet(nn.Module):\n",
        "    def __init__(self, num_classes, encoder=None):  #use encoder to pass pretrained encoder\n",
        "        super().__init__()\n",
        "\n",
        "        if (encoder == None):\n",
        "            self.encoder = Encoder(num_classes)\n",
        "        else:\n",
        "            self.encoder = encoder\n",
        "        self.decoder = Decoder(num_classes)\n",
        "\n",
        "    def forward(self, input, only_encode=False):\n",
        "        if only_encode:\n",
        "            return self.encoder.forward(input, predict=True)\n",
        "        else:\n",
        "            output = self.encoder(input)    #predict=False by default\n",
        "            return self.decoder.forward(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahMrTxUB6Qn7"
      },
      "source": [
        "#iou eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06MZNTLV6SUr"
      },
      "source": [
        "# Code for evaluating IoU \n",
        "# Nov 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import torch\n",
        "\n",
        "class iouEval:\n",
        "\n",
        "    def __init__(self, nClasses, ignoreIndex=19):\n",
        "        self.nClasses = nClasses\n",
        "        self.ignoreIndex = ignoreIndex if nClasses>ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\n",
        "        self.reset()\n",
        "\n",
        "    def reset (self):\n",
        "        classes = self.nClasses if self.ignoreIndex==-1 else self.nClasses-1\n",
        "        self.tp = torch.zeros(classes).double()\n",
        "        self.fp = torch.zeros(classes).double()\n",
        "        self.fn = torch.zeros(classes).double()        \n",
        "\n",
        "    def addBatch(self, x, y):   #x=preds, y=targets\n",
        "        #sizes should be \"batch_size x nClasses x H x W\"\n",
        "        \n",
        "        #print (\"X is cuda: \", x.is_cuda)\n",
        "        #print (\"Y is cuda: \", y.is_cuda)\n",
        "\n",
        "        if (x.is_cuda or y.is_cuda):\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "        #if size is \"batch_size x 1 x H x W\" scatter to onehot\n",
        "        if (x.size(1) == 1):\n",
        "            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \n",
        "            if x.is_cuda:\n",
        "                x_onehot = x_onehot.cuda()\n",
        "            x_onehot.scatter_(1, x, 1).float()\n",
        "        else:\n",
        "            x_onehot = x.float()\n",
        "\n",
        "        if (y.size(1) == 1):\n",
        "            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\n",
        "            if y.is_cuda:\n",
        "                y_onehot = y_onehot.cuda()\n",
        "            y_onehot.scatter_(1, y, 1).float()\n",
        "        else:\n",
        "            y_onehot = y.float()\n",
        "\n",
        "        if (self.ignoreIndex != -1): \n",
        "            ignores = y_onehot[:,self.ignoreIndex].unsqueeze(1)\n",
        "            x_onehot = x_onehot[:, :self.ignoreIndex]\n",
        "            y_onehot = y_onehot[:, :self.ignoreIndex]\n",
        "        else:\n",
        "            ignores=0\n",
        "\n",
        "        #print(type(x_onehot))\n",
        "        #print(type(y_onehot))\n",
        "        #print(x_onehot.size())\n",
        "        #print(y_onehot.size())\n",
        "\n",
        "        tpmult = x_onehot * y_onehot    #times prediction and gt coincide is 1\n",
        "        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n",
        "        fpmult = x_onehot * (1-y_onehot-ignores) #times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\n",
        "        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n",
        "        fnmult = (1-x_onehot) * (y_onehot) #times prediction says its not that class and gt says it is\n",
        "        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \n",
        "\n",
        "        self.tp += tp.double().cpu()\n",
        "        self.fp += fp.double().cpu()\n",
        "        self.fn += fn.double().cpu()\n",
        "\n",
        "    def getIoU(self):\n",
        "        num = self.tp\n",
        "        den = self.tp + self.fp + self.fn + 1e-15\n",
        "        iou = num / den\n",
        "        return torch.mean(iou), iou     #returns \"iou mean\", \"iou per class\"\n",
        "\n",
        "# Class for colors\n",
        "class colors:\n",
        "    RED       = '\\033[31;1m'\n",
        "    GREEN     = '\\033[32;1m'\n",
        "    YELLOW    = '\\033[33;1m'\n",
        "    BLUE      = '\\033[34;1m'\n",
        "    MAGENTA   = '\\033[35;1m'\n",
        "    CYAN      = '\\033[36;1m'\n",
        "    BOLD      = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    ENDC      = '\\033[0m'\n",
        "\n",
        "# Colored value output if colorized flag is activated.\n",
        "def getColorEntry(val):\n",
        "    if not isinstance(val, float):\n",
        "        return colors.ENDC\n",
        "    if (val < .20):\n",
        "        return colors.RED\n",
        "    elif (val < .40):\n",
        "        return colors.YELLOW\n",
        "    elif (val < .60):\n",
        "        return colors.BLUE\n",
        "    elif (val < .80):\n",
        "        return colors.CYAN\n",
        "    else:\n",
        "        return colors.GREEN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ0PAlGR6Tl4"
      },
      "source": [
        "#transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21pctpv46VE9"
      },
      "source": [
        "# Code with transformations for Cityscapes (adapted from bodokaiser/piwise code)\n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def colormap_cityscapes(n):\n",
        "    cmap=np.zeros([n, 3]).astype(np.uint8)\n",
        "    cmap[0,:] = np.array([128, 64,128])\n",
        "    cmap[1,:] = np.array([244, 35,232])\n",
        "    cmap[2,:] = np.array([ 70, 70, 70])\n",
        "    cmap[3,:] = np.array([ 102,102,156])\n",
        "    cmap[4,:] = np.array([ 190,153,153])\n",
        "    cmap[5,:] = np.array([ 153,153,153])\n",
        "\n",
        "    cmap[6,:] = np.array([ 250,170, 30])\n",
        "    cmap[7,:] = np.array([ 220,220,  0])\n",
        "    cmap[8,:] = np.array([ 107,142, 35])\n",
        "    cmap[9,:] = np.array([ 152,251,152])\n",
        "    cmap[10,:] = np.array([ 70,130,180])\n",
        "\n",
        "    cmap[11,:] = np.array([ 220, 20, 60])\n",
        "    cmap[12,:] = np.array([ 255,  0,  0])\n",
        "    cmap[13,:] = np.array([ 0,  0,142])\n",
        "    cmap[14,:] = np.array([  0,  0, 70])\n",
        "    cmap[15,:] = np.array([  0, 60,100])\n",
        "\n",
        "    cmap[16,:] = np.array([  0, 80,100])\n",
        "    cmap[17,:] = np.array([  0,  0,230])\n",
        "    cmap[18,:] = np.array([ 119, 11, 32])\n",
        "    cmap[19,:] = np.array([ 0,  0,  0])\n",
        "    \n",
        "    return cmap\n",
        "\n",
        "\n",
        "def colormap(n):\n",
        "    cmap=np.zeros([n, 3]).astype(np.uint8)\n",
        "\n",
        "    for i in np.arange(n):\n",
        "        r, g, b = np.zeros(3)\n",
        "\n",
        "        for j in np.arange(8):\n",
        "            r = r + (1<<(7-j))*((i&(1<<(3*j))) >> (3*j))\n",
        "            g = g + (1<<(7-j))*((i&(1<<(3*j+1))) >> (3*j+1))\n",
        "            b = b + (1<<(7-j))*((i&(1<<(3*j+2))) >> (3*j+2))\n",
        "\n",
        "        cmap[i,:] = np.array([r, g, b])\n",
        "\n",
        "    return cmap\n",
        "\n",
        "class Relabel:\n",
        "\n",
        "    def __init__(self, olabel, nlabel):\n",
        "        self.olabel = olabel\n",
        "        self.nlabel = nlabel\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        assert isinstance(tensor, torch.LongTensor) or isinstance(tensor, torch.ByteTensor) , 'tensor needs to be LongTensor'\n",
        "        tensor[tensor == self.olabel] = self.nlabel\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class ToLabel:\n",
        "\n",
        "    def __call__(self, image):\n",
        "        return torch.from_numpy(np.array(image)).long().unsqueeze(0)\n",
        "\n",
        "\n",
        "class Colorize:\n",
        "\n",
        "    def __init__(self, n=22):\n",
        "        #self.cmap = colormap(256)\n",
        "        self.cmap = colormap_cityscapes(256)\n",
        "        self.cmap[n] = self.cmap[-1]\n",
        "        self.cmap = torch.from_numpy(self.cmap[:n])\n",
        "\n",
        "    def __call__(self, gray_image):\n",
        "        size = gray_image.size()\n",
        "        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n",
        "\n",
        "        #for label in range(1, len(self.cmap)):\n",
        "        for label in range(0, len(self.cmap)):\n",
        "            mask = gray_image[0] == label\n",
        "\n",
        "            color_image[0][mask] = self.cmap[label][0]\n",
        "            color_image[1][mask] = self.cmap[label][1]\n",
        "            color_image[2][mask] = self.cmap[label][2]\n",
        "\n",
        "        return color_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MNKuVrs6gRz"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr7zMh3p6h3x"
      },
      "source": [
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize((512,1024),Image.BILINEAR),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize((512,1024),Image.NEAREST),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "])\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    modelpath = args.loadDir + args.loadModel\n",
        "    weightspath = args.loadDir + args.loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not args.cpu):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(args.datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(args.datadir, input_transform_cityscapes, target_transform_cityscapes, subset=args.subset),\n",
        "        num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    # For visualizer:\n",
        "    # must launch in other window \"python3.6 -m visdom.server -port 8097\"\n",
        "    # and access localhost:8097 to see it\n",
        "    if (args.visualize):\n",
        "        vis = visdom.Visdom()\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        if (not args.cpu):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images)\n",
        "        #targets = Variable(labels)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        #label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        label_color = Colorize()(label.unsqueeze(0))\n",
        "\n",
        "        filenameSave = \"/content/gdrive/MyDrive/save/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)      \n",
        "        label_save = ToPILImage()(label_color)           \n",
        "        label_save.save(filenameSave) \n",
        "\n",
        "        if (args.visualize):\n",
        "            vis.image(label_color.numpy())\n",
        "        print (step, filenameSave)\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--state')\n",
        "\n",
        "    parser.add_argument('--loadDir',default=\"/content/gdrive/MyDrive/\")\n",
        "    parser.add_argument('--loadWeights', default=\"model_best.pth\")\n",
        "    parser.add_argument('--loadModel', default=\"erfnet.py\")\n",
        "    parser.add_argument('--subset', default=\"val\")  #can be val, test, train, demoSequence\n",
        "\n",
        "    parser.add_argument('--datadir', default=\"/content/gdrive/MyDrive/cityscapes\")\n",
        "    parser.add_argument('--num-workers', type=int, default=4)\n",
        "    parser.add_argument('--batch-size', type=int, default=1)\n",
        "    parser.add_argument('--cpu', action='store_true', default=False)\n",
        "\n",
        "    parser.add_argument('--visualize', action='store_true', default=True)\n",
        "    main(parser.parse_args(''))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}